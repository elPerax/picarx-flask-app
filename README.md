# ğŸŒ Picar-X Flask Dashboard  

**Course:** Internet of Things 2 (IoT 2) â€“ Fall 2025  
**Student:** Samuel Reyes Cifuentes  
**Institution:** Champlain College Saint-Lambert  

---

## ğŸš€ Project Overview

This repository contains the **Flask-based web dashboard** used to interact with the **VisionTrack-X PiCarX robot**.  
The dashboard provides a full cloud-connected interface to:

- Monitor the rover in **real time**  
- Visualize **historical telemetry** from Neon PostgreSQL  
- Remotely control **motors, steering, camera, and TTS**

This app is the **visualization & control layer** of the IoT pipeline:

> **Sense â†’ Publish (MQTT) â†’ Store â†’ Sync â†’ Visualize â†’ Control**

The PiCarX handles **sensing + MQTT**,  
the Flask app handles **visualization + remote control**.

---


## ğŸ”— Related Repository (Full PiCar-X System)

ğŸ‘‰ **Main PiCar-X Project Repository:**  
https://github.com/elPerax/Picar-X-v2.0-IoT-Smart-Robot-Car

This Flask dashboard is the *cloud & visualization component* of that full system.

---

## ğŸ¥ Youtube video Link for Milestome 3
https://www.youtube.com/watch?v=Gjl8jm351ow

---

## ğŸ§  Features Implemented

### âœ… Live Sensor Dashboard
- Real-time **ultrasonic distance** graph  
- Real-time **grayscale mid-value** graph  
- Shows the **latest TTS message** spoken by the robot  
- Data pulled directly from **Adafruit IO REST API**

### âœ… Historical Data Visualization
- Plots **ultrasonic** and **grayscale** telemetry stored in **Neon PostgreSQL**  
- Date range selection / filtering  
- Uses **Chart.js** for interactive, responsive graphs  

### âœ… Remote Robot Controls
- **Motor control:** forward / backward / stop  
- **Steering control:** left / right / center  
- **Camera control:** pan / tilt  
- **Text-to-Speech:** send phrases to the robot  
- **Line tracking:** start / stop  
- **Obstacle avoidance:** start / stop  

Commands are sent through **Adafruit IO command feeds**.

### âœ… About Page
- Overview of **hardware + software stack**  
- Short descriptions of **team members** (no personal photos)  
- Includes the **official VisionTrack-X car photo**

### âœ… Modern UI (Final-Project Ready)
- Custom **dark theme** (`style.css`)  
- Hero banner with **logo + car image**  
- Centered layout, clean typography, responsive design  
- Fully deployable on **Render**

---

## âš™ï¸ System Architecture (Dashboard Perspective)

```text
Sensors (Ultrasonic, Grayscale, TTS)
        â†“
 Adafruit IO Feeds (MQTT / REST)
        â†“
   Flask Web Dashboard (Render)
        â†“
Neon PostgreSQL (Historical Data)
        â†“
Robot Control â†’ Commands published to AIO feeds

PiCar-X Robot
    â†’ publishes live values â†’ Adafruit IO
            â†“
Flask Dashboard (this repo)
    â†’ reads live values from Adafruit IO
    â†’ reads historical logs from Neon
    â†’ sends control commands back to AIO
```

##  ğŸ“ Directory Structure
```picarx-flask-app/
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ style.css           # Full UI theme
â”‚   â”œâ”€â”€ js/
â”‚   â”‚   â”œâ”€â”€ charts_ultra.js     # Ultrasonic historical charts
â”‚   â”‚   â””â”€â”€ charts_gray.js      # Grayscale historical charts
â”‚   â””â”€â”€ img/                    # Car / logo images
â”‚
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ base.html               # Navbar + shared layout
â”‚   â”œâ”€â”€ home.html               # Live dashboard
â”‚   â”œâ”€â”€ about.html              # About page
â”‚   â”œâ”€â”€ sensor_data.html
â”‚   â”œâ”€â”€ charts_ultra.html
â”‚   â”œâ”€â”€ charts_gray.html
â”‚   â”œâ”€â”€ control_motors.html
â”‚   â”œâ”€â”€ control_steering.html
â”‚   â”œâ”€â”€ camera.html
â”‚   â”œâ”€â”€ tts_control.html
â”‚   â”œâ”€â”€ line_tracking.html
â”‚   â””â”€â”€ obstacle.html
â”‚
â”œâ”€â”€ app.py                      # Flask routes + Neon + AIO integration
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ runtime.txt                 # Python version (for Render)
â”œâ”€â”€ Procfile                    # Gunicorn startup command
â””â”€â”€ .gitignore                  # .env and other local-only files
```
## ğŸ”§ Configuration
Create a .env file (not committed to Git) with:
```AIO_USERNAME=your_adafruit_username
AIO_KEY=your_adafruit_key

AIO_ULTRASONIC_FEED=ultrasonic_distance
AIO_GRAYSCALE_MID_FEED=grayscale_mid
AIO_TTS_FEED=tts
AIO_COMMAND_FEED=picarx-command
AIO_STEERING_FEED=steering-command
AIO_CAMERA_FEED=camera-command

PG_DSN=postgresql://username:password@host:5432/dbname?sslmode=require
```

## ğŸ§© How to Run Locally

```# 1ï¸âƒ£ Clone the repository
git clone https://github.com/<yourname>/picarx-flask-app.git
cd picarx-flask-app

# 2ï¸âƒ£ Create and activate a virtual environment
python -m venv venv

# Windows:
venv\Scripts\activate

# macOS / Linux:
# source venv/bin/activate

# 3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

# 4ï¸âƒ£ Create `.env` file (see configuration section)

# 5ï¸âƒ£ Run the Flask app
python app.py
Then open the dashboard in your browser:

text
Copy code
http://127.0.0.1:5000
```
## ğŸŒ Deployment on Render

This repo is ready for Render:

âœ” Procfile included

âœ” runtime.txt included

âœ” Gunicorn entrypoint configured

Deployment steps:

Push this repository to GitHub.

In Render, create a new Web Service.

Connect it to this repo and select the correct branch (e.g. main).

Add all environment variables from the .env section.

Click Deploy.

Render will:

Use the Python version from runtime.txt

Install packages from requirements.txt

Start the app using Gunicorn (command in Procfile)

After each commit, you can either rely on Auto Deploys or trigger a Manual Deploy â†’ Latest Commit.

## ğŸ“Š Example Live Data (as seen on the dashboard)
Timestamp:      2025-11-30 23:51:10
Ultrasonic:     42 cm
Grayscale Mid:  533
Last TTS:       "Hello, this is VisionTrack-X!"

## ğŸ§° Tools & Libraries

Flask â€“ Web server framework

requests â€“ HTTP client for Adafruit IO REST API

Chart.js â€“ Front-end charting library

Gunicorn â€“ Production WSGI server (Render)

Neon PostgreSQL â€“ Cloud database for historical data

Render â€“ Hosting platform + CI/CD

## ğŸ§  Reflection


Working on the VisionTrack-X Flask Dashboard was the final step in bringing the entire IoT ecosystem together.  
Throughout this project, I built both sides of the system:

- The **hardware & sensing layer** (PiCar-X robot, sensors, actuators)  
- The **cloud & visualization layer** (Flask dashboard, Adafruit IO, Neon)  

Because of this, I got to fully understand how an IoT solution works from end to end.

The first part of the project â€” wiring, calibrating, and coding the PiCar-X â€” was extremely hands-on.  
I built scripts to control ultrasonic sensing, grayscale line tracking, DHT11 measurements, TTS audio, servo steering, and motor movement.  
I also configured MQTT publishing to Adafruit IO, structured local CSV logging, and automated nightly uploads to Google Drive.  
This stage forced me to deeply understand hardware behavior, sensor timing, asynchronous events, and the constraints of real devices.

The Flask dashboard represented the second half of the ecosystem.  
Here, the challenge shifted from hardware to **software architecture and cloud integration**.  
I had to make a UI that felt like a real control interfaceâ€”not a school assignmentâ€”and integrate it with:

- Adafruit IO REST APIs (live sensor values + command feeds)  
- Neon PostgreSQL (historical logs)  
- A responsive front-end (Chart.js, CSS)  
- Flask routes, templates, and cloud deployment via Render  

This required careful planning: handling JSON payloads, ensuring the charts updated cleanly, preventing broken layouts, and keeping everything responsive.  
Building the control pages (motors, steering, camera, TTS, line tracking, obstacle avoidance) taught me how to design a safe, reliable command interface that would not accidentally send repeated or conflicting signals to the robot.

What made this dashboard meaningful is that I had already completed the PiCar-X hardware system.  
I wasnâ€™t just designing a UI â€” I was building the missing half of a connected IoT product.  
Seeing the robot publish data through MQTT, watching the dashboard plot that same data, and then sending commands back from the dashboard to the robot completed the full loop:

**Device â†’ Cloud â†’ Dashboard â†’ Device**

This final stage made everything click.  
It showed me how real-world IoT devices use multiple layers â€” hardware, networking, cloud storage, APIs, and visualization â€” to create a complete system.  
By the end, the VisionTrack-X ecosystem felt like a coherent product:  
the sensing scripts, logging pipeline, MQTT feeds, cloud database, and dashboard all worked together seamlessly.

Overall, this project helped me understand IoT not just as â€œa robot with sensors,â€ but as an integrated system made up of hardware, software, cloud pipelines, and user-facing dashboards.  
It was challenging, but seeing the robot respond to cloud commands and watching live values graph in real time made the entire process extremely rewarding.
